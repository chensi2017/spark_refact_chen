from kafka: {"tid":60,"ts":1529520445364,"data":[{"k":"pc","dn":"0000003C000037040000000000012808","v":1963203073,"ts":1529520445000},{"k":"pe","dn":"0000003C000037040000000000012809","v":2000028553,"ts":1529520445000},{"k":"chill_ewt","dn":"0000003C00003704000000000001280A","v":1242801208,"ts":1529520445000},{"k":"chill_lwt","dn":"0000003C00003704000000000001280B","v":668212268,"ts":1529520445000},{"k":"cool_ewt","dn":"0000003C00003704000000000001280C","v":1868450386,"ts":1529520445000},{"k":"cool_lwt","dn":"0000003C00003704000000000001280D","v":606334002,"ts":1529520445000},{"k":"tc","dn":"0000003C00003704000000000001280E","v":478089040,"ts":1529520445000},{"k":"te","dn":"0000003C00003704000000000001280F","v":1684169812,"ts":1529520445000},{"k":"compmotorcurr","dn":"0000003C000037040000000000012810","v":458878907,"ts":1529520445000},{"k":"unitload","dn":"0000003C000037040000000000012811","v":1720890249,"ts":1529520445000}],"did":"14084"}
to kafka:   {"eventId":282,"dn":"AAAAPAAAZmcAAAAAAAMEDQ==","ts":1529576333000,"value":5.0979412E7,"status":1}
redis阈值格式:key->event_<metricId>_<metricName> value->[{"id":962,"op":"=","min":120.0,"max":0.0},{"id":963,"op":"=","min":120.0,"max":0.0},{"id":964,"op":"=","min":120.0,"max":0.0},{"id":965,"op":"=","min":120.0,"max":0.0},{"id":966,"op":"=","min":120.0,"max":0.0},{"id":967,"op":"=","min":120.0,"max":0.0},{"id":968,"op":"=","min":120.0,"max":0.0}]



======================================================================================================================================================================================
需求：
所有独立的测点数
按日期统计独立的测点数
按租户和日期统计独立的测点数
按时间窗口和租户统计独立的测点数
按窗口统计所有独立的测点数
实现方法：
使用mapWithState算子进行去重和状态维护 k:Long S:(Int,Int) E:(Long,String,Int,Boolean,Int)
输入:(metricId:Long,(ts:Long,tid:String,count:Int))
State:(times:Int,date:Int)
输出:(metricId:Long,tid:String,times:Int,firstAppear:Boolean,date,Int)
Redis中存储形式{
    --------独立测点--------
    1.所有独立测点数=>key: htstream:unique:dp:total
    2.按日期统计独立的测点数=>key: htstream:unique:dp:by:date filed: <20180629>
    3.按租户和日期统计独立的测点数=>key: htstream:unique:dp:by:tenantid:date filed: <20180629:tid>
    del htstream:unique:dp:by:tenantid:date htstream:unique:dp:by:date htstream:unique:dp:by:tenantid:window htstream:unique:dp:total:by:window htstream:unique:dp:total
    --------所有测点--------
    4.所有测点数=>key: htstream:total:dp
    5.按租户统计所有测点数=>key: htstream:total:dp:tenanid:<tid>
    6.按日期统计测点数=>key: htstream:total:dp:date:<date>  //egg:htstream:total:dp:date:2018-08-06
    7.按date和tid统计测点数=>key: htstream:total:dp:tenantid:<tid>:date:<date>

}
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
窗口操作
Redis中存储形式{
    按时间窗口和租户统计独立的测点数=>key: htstream:unique:dp:by:tenantid:window filed: <tid>
    按窗口统计所有独立的测点数=>key: htstream:unique:dp:total:by:window
}
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
状态恢复
利用checkpoint存储的rdd数据恢复mapWithState算子中得状态数据(checkpoint addr:hdfs://192.168.0.78:8020/...)
